Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'cls.predictions.decoder.bias', 'bert.pooler.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Using device cuda:0
Classification_dim: 256
427155714
##  bert_encoder.bert.embeddings.word_embeddings.weight True
##  bert_encoder.bert.embeddings.position_embeddings.weight True
##  bert_encoder.bert.embeddings.token_type_embeddings.weight True
##  bert_encoder.bert.embeddings.LayerNorm.weight True
##  bert_encoder.bert.embeddings.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.0.attention.self.query.weight True
##  bert_encoder.bert.encoder.layer.0.attention.self.query.bias True
##  bert_encoder.bert.encoder.layer.0.attention.self.key.weight True
##  bert_encoder.bert.encoder.layer.0.attention.self.key.bias True
##  bert_encoder.bert.encoder.layer.0.attention.self.value.weight True
##  bert_encoder.bert.encoder.layer.0.attention.self.value.bias True
##  bert_encoder.bert.encoder.layer.0.attention.output.dense.weight True
##  bert_encoder.bert.encoder.layer.0.attention.output.dense.bias True
##  bert_encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.0.intermediate.dense.weight True
##  bert_encoder.bert.encoder.layer.0.intermediate.dense.bias True
##  bert_encoder.bert.encoder.layer.0.output.dense.weight True
##  bert_encoder.bert.encoder.layer.0.output.dense.bias True
##  bert_encoder.bert.encoder.layer.0.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.0.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.1.attention.self.query.weight True
##  bert_encoder.bert.encoder.layer.1.attention.self.query.bias True
##  bert_encoder.bert.encoder.layer.1.attention.self.key.weight True
##  bert_encoder.bert.encoder.layer.1.attention.self.key.bias True
##  bert_encoder.bert.encoder.layer.1.attention.self.value.weight True
##  bert_encoder.bert.encoder.layer.1.attention.self.value.bias True
##  bert_encoder.bert.encoder.layer.1.attention.output.dense.weight True
##  bert_encoder.bert.encoder.layer.1.attention.output.dense.bias True
##  bert_encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.1.intermediate.dense.weight True
##  bert_encoder.bert.encoder.layer.1.intermediate.dense.bias True
##  bert_encoder.bert.encoder.layer.1.output.dense.weight True
##  bert_encoder.bert.encoder.layer.1.output.dense.bias True
##  bert_encoder.bert.encoder.layer.1.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.1.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.2.attention.self.query.weight True
##  bert_encoder.bert.encoder.layer.2.attention.self.query.bias True
##  bert_encoder.bert.encoder.layer.2.attention.self.key.weight True
##  bert_encoder.bert.encoder.layer.2.attention.self.key.bias True
##  bert_encoder.bert.encoder.layer.2.attention.self.value.weight True
##  bert_encoder.bert.encoder.layer.2.attention.self.value.bias True
##  bert_encoder.bert.encoder.layer.2.attention.output.dense.weight True
##  bert_encoder.bert.encoder.layer.2.attention.output.dense.bias True
##  bert_encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.2.intermediate.dense.weight True
##  bert_encoder.bert.encoder.layer.2.intermediate.dense.bias True
##  bert_encoder.bert.encoder.layer.2.output.dense.weight True
##  bert_encoder.bert.encoder.layer.2.output.dense.bias True
##  bert_encoder.bert.encoder.layer.2.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.2.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.3.attention.self.query.weight True
##  bert_encoder.bert.encoder.layer.3.attention.self.query.bias True
##  bert_encoder.bert.encoder.layer.3.attention.self.key.weight True
##  bert_encoder.bert.encoder.layer.3.attention.self.key.bias True
##  bert_encoder.bert.encoder.layer.3.attention.self.value.weight True
##  bert_encoder.bert.encoder.layer.3.attention.self.value.bias True
##  bert_encoder.bert.encoder.layer.3.attention.output.dense.weight True
##  bert_encoder.bert.encoder.layer.3.attention.output.dense.bias True
##  bert_encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.3.intermediate.dense.weight True
##  bert_encoder.bert.encoder.layer.3.intermediate.dense.bias True
##  bert_encoder.bert.encoder.layer.3.output.dense.weight True
##  bert_encoder.bert.encoder.layer.3.output.dense.bias True
##  bert_encoder.bert.encoder.layer.3.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.3.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.4.attention.self.query.weight True
##  bert_encoder.bert.encoder.layer.4.attention.self.query.bias True
##  bert_encoder.bert.encoder.layer.4.attention.self.key.weight True
##  bert_encoder.bert.encoder.layer.4.attention.self.key.bias True
##  bert_encoder.bert.encoder.layer.4.attention.self.value.weight True
##  bert_encoder.bert.encoder.layer.4.attention.self.value.bias True
##  bert_encoder.bert.encoder.layer.4.attention.output.dense.weight True
##  bert_encoder.bert.encoder.layer.4.attention.output.dense.bias True
##  bert_encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.4.intermediate.dense.weight True
##  bert_encoder.bert.encoder.layer.4.intermediate.dense.bias True
##  bert_encoder.bert.encoder.layer.4.output.dense.weight True
##  bert_encoder.bert.encoder.layer.4.output.dense.bias True
##  bert_encoder.bert.encoder.layer.4.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.4.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.5.attention.self.query.weight True
##  bert_encoder.bert.encoder.layer.5.attention.self.query.bias True
##  bert_encoder.bert.encoder.layer.5.attention.self.key.weight True
##  bert_encoder.bert.encoder.layer.5.attention.self.key.bias True
##  bert_encoder.bert.encoder.layer.5.attention.self.value.weight True
##  bert_encoder.bert.encoder.layer.5.attention.self.value.bias True
##  bert_encoder.bert.encoder.layer.5.attention.output.dense.weight True
##  bert_encoder.bert.encoder.layer.5.attention.output.dense.bias True
##  bert_encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.5.intermediate.dense.weight True
##  bert_encoder.bert.encoder.layer.5.intermediate.dense.bias True
##  bert_encoder.bert.encoder.layer.5.output.dense.weight True
##  bert_encoder.bert.encoder.layer.5.output.dense.bias True
##  bert_encoder.bert.encoder.layer.5.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.5.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.6.attention.self.query.weight True
##  bert_encoder.bert.encoder.layer.6.attention.self.query.bias True
##  bert_encoder.bert.encoder.layer.6.attention.self.key.weight True
##  bert_encoder.bert.encoder.layer.6.attention.self.key.bias True
##  bert_encoder.bert.encoder.layer.6.attention.self.value.weight True
##  bert_encoder.bert.encoder.layer.6.attention.self.value.bias True
##  bert_encoder.bert.encoder.layer.6.attention.output.dense.weight True
##  bert_encoder.bert.encoder.layer.6.attention.output.dense.bias True
##  bert_encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.6.intermediate.dense.weight True
##  bert_encoder.bert.encoder.layer.6.intermediate.dense.bias True
##  bert_encoder.bert.encoder.layer.6.output.dense.weight True
##  bert_encoder.bert.encoder.layer.6.output.dense.bias True
##  bert_encoder.bert.encoder.layer.6.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.6.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.7.attention.self.query.weight True
##  bert_encoder.bert.encoder.layer.7.attention.self.query.bias True
##  bert_encoder.bert.encoder.layer.7.attention.self.key.weight True
##  bert_encoder.bert.encoder.layer.7.attention.self.key.bias True
##  bert_encoder.bert.encoder.layer.7.attention.self.value.weight True
##  bert_encoder.bert.encoder.layer.7.attention.self.value.bias True
##  bert_encoder.bert.encoder.layer.7.attention.output.dense.weight True
##  bert_encoder.bert.encoder.layer.7.attention.output.dense.bias True
##  bert_encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.7.intermediate.dense.weight True
##  bert_encoder.bert.encoder.layer.7.intermediate.dense.bias True
##  bert_encoder.bert.encoder.layer.7.output.dense.weight True
##  bert_encoder.bert.encoder.layer.7.output.dense.bias True
##  bert_encoder.bert.encoder.layer.7.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.7.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.8.attention.self.query.weight True
##  bert_encoder.bert.encoder.layer.8.attention.self.query.bias True
##  bert_encoder.bert.encoder.layer.8.attention.self.key.weight True
##  bert_encoder.bert.encoder.layer.8.attention.self.key.bias True
##  bert_encoder.bert.encoder.layer.8.attention.self.value.weight True
##  bert_encoder.bert.encoder.layer.8.attention.self.value.bias True
##  bert_encoder.bert.encoder.layer.8.attention.output.dense.weight True
##  bert_encoder.bert.encoder.layer.8.attention.output.dense.bias True
##  bert_encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.8.intermediate.dense.weight True
##  bert_encoder.bert.encoder.layer.8.intermediate.dense.bias True
##  bert_encoder.bert.encoder.layer.8.output.dense.weight True
##  bert_encoder.bert.encoder.layer.8.output.dense.bias True
##  bert_encoder.bert.encoder.layer.8.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.8.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.9.attention.self.query.weight True
##  bert_encoder.bert.encoder.layer.9.attention.self.query.bias True
##  bert_encoder.bert.encoder.layer.9.attention.self.key.weight True
##  bert_encoder.bert.encoder.layer.9.attention.self.key.bias True
##  bert_encoder.bert.encoder.layer.9.attention.self.value.weight True
##  bert_encoder.bert.encoder.layer.9.attention.self.value.bias True
##  bert_encoder.bert.encoder.layer.9.attention.output.dense.weight True
##  bert_encoder.bert.encoder.layer.9.attention.output.dense.bias True
##  bert_encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.9.intermediate.dense.weight True
##  bert_encoder.bert.encoder.layer.9.intermediate.dense.bias True
##  bert_encoder.bert.encoder.layer.9.output.dense.weight True
##  bert_encoder.bert.encoder.layer.9.output.dense.bias True
##  bert_encoder.bert.encoder.layer.9.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.9.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.10.attention.self.query.weight True
##  bert_encoder.bert.encoder.layer.10.attention.self.query.bias True
##  bert_encoder.bert.encoder.layer.10.attention.self.key.weight True
##  bert_encoder.bert.encoder.layer.10.attention.self.key.bias True
##  bert_encoder.bert.encoder.layer.10.attention.self.value.weight True
##  bert_encoder.bert.encoder.layer.10.attention.self.value.bias True
##  bert_encoder.bert.encoder.layer.10.attention.output.dense.weight True
##  bert_encoder.bert.encoder.layer.10.attention.output.dense.bias True
##  bert_encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.10.intermediate.dense.weight True
##  bert_encoder.bert.encoder.layer.10.intermediate.dense.bias True
##  bert_encoder.bert.encoder.layer.10.output.dense.weight True
##  bert_encoder.bert.encoder.layer.10.output.dense.bias True
##  bert_encoder.bert.encoder.layer.10.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.10.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.11.attention.self.query.weight True
##  bert_encoder.bert.encoder.layer.11.attention.self.query.bias True
##  bert_encoder.bert.encoder.layer.11.attention.self.key.weight True
##  bert_encoder.bert.encoder.layer.11.attention.self.key.bias True
##  bert_encoder.bert.encoder.layer.11.attention.self.value.weight True
##  bert_encoder.bert.encoder.layer.11.attention.self.value.bias True
##  bert_encoder.bert.encoder.layer.11.attention.output.dense.weight True
##  bert_encoder.bert.encoder.layer.11.attention.output.dense.bias True
##  bert_encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.11.intermediate.dense.weight True
##  bert_encoder.bert.encoder.layer.11.intermediate.dense.bias True
##  bert_encoder.bert.encoder.layer.11.output.dense.weight True
##  bert_encoder.bert.encoder.layer.11.output.dense.bias True
##  bert_encoder.bert.encoder.layer.11.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.11.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.12.attention.self.query.weight True
##  bert_encoder.bert.encoder.layer.12.attention.self.query.bias True
##  bert_encoder.bert.encoder.layer.12.attention.self.key.weight True
##  bert_encoder.bert.encoder.layer.12.attention.self.key.bias True
##  bert_encoder.bert.encoder.layer.12.attention.self.value.weight True
##  bert_encoder.bert.encoder.layer.12.attention.self.value.bias True
##  bert_encoder.bert.encoder.layer.12.attention.output.dense.weight True
##  bert_encoder.bert.encoder.layer.12.attention.output.dense.bias True
##  bert_encoder.bert.encoder.layer.12.attention.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.12.attention.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.12.intermediate.dense.weight True
##  bert_encoder.bert.encoder.layer.12.intermediate.dense.bias True
##  bert_encoder.bert.encoder.layer.12.output.dense.weight True
##  bert_encoder.bert.encoder.layer.12.output.dense.bias True
##  bert_encoder.bert.encoder.layer.12.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.12.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.13.attention.self.query.weight True
##  bert_encoder.bert.encoder.layer.13.attention.self.query.bias True
##  bert_encoder.bert.encoder.layer.13.attention.self.key.weight True
##  bert_encoder.bert.encoder.layer.13.attention.self.key.bias True
##  bert_encoder.bert.encoder.layer.13.attention.self.value.weight True
##  bert_encoder.bert.encoder.layer.13.attention.self.value.bias True
##  bert_encoder.bert.encoder.layer.13.attention.output.dense.weight True
##  bert_encoder.bert.encoder.layer.13.attention.output.dense.bias True
##  bert_encoder.bert.encoder.layer.13.attention.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.13.attention.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.13.intermediate.dense.weight True
##  bert_encoder.bert.encoder.layer.13.intermediate.dense.bias True
##  bert_encoder.bert.encoder.layer.13.output.dense.weight True
##  bert_encoder.bert.encoder.layer.13.output.dense.bias True
##  bert_encoder.bert.encoder.layer.13.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.13.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.14.attention.self.query.weight True
##  bert_encoder.bert.encoder.layer.14.attention.self.query.bias True
##  bert_encoder.bert.encoder.layer.14.attention.self.key.weight True
##  bert_encoder.bert.encoder.layer.14.attention.self.key.bias True
##  bert_encoder.bert.encoder.layer.14.attention.self.value.weight True
##  bert_encoder.bert.encoder.layer.14.attention.self.value.bias True
##  bert_encoder.bert.encoder.layer.14.attention.output.dense.weight True
##  bert_encoder.bert.encoder.layer.14.attention.output.dense.bias True
##  bert_encoder.bert.encoder.layer.14.attention.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.14.attention.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.14.intermediate.dense.weight True
##  bert_encoder.bert.encoder.layer.14.intermediate.dense.bias True
##  bert_encoder.bert.encoder.layer.14.output.dense.weight True
##  bert_encoder.bert.encoder.layer.14.output.dense.bias True
##  bert_encoder.bert.encoder.layer.14.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.14.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.15.attention.self.query.weight True
##  bert_encoder.bert.encoder.layer.15.attention.self.query.bias True
##  bert_encoder.bert.encoder.layer.15.attention.self.key.weight True
##  bert_encoder.bert.encoder.layer.15.attention.self.key.bias True
##  bert_encoder.bert.encoder.layer.15.attention.self.value.weight True
##  bert_encoder.bert.encoder.layer.15.attention.self.value.bias True
##  bert_encoder.bert.encoder.layer.15.attention.output.dense.weight True
##  bert_encoder.bert.encoder.layer.15.attention.output.dense.bias True
##  bert_encoder.bert.encoder.layer.15.attention.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.15.attention.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.15.intermediate.dense.weight True
##  bert_encoder.bert.encoder.layer.15.intermediate.dense.bias True
##  bert_encoder.bert.encoder.layer.15.output.dense.weight True
##  bert_encoder.bert.encoder.layer.15.output.dense.bias True
##  bert_encoder.bert.encoder.layer.15.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.15.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.16.attention.self.query.weight True
##  bert_encoder.bert.encoder.layer.16.attention.self.query.bias True
##  bert_encoder.bert.encoder.layer.16.attention.self.key.weight True
##  bert_encoder.bert.encoder.layer.16.attention.self.key.bias True
##  bert_encoder.bert.encoder.layer.16.attention.self.value.weight True
##  bert_encoder.bert.encoder.layer.16.attention.self.value.bias True
##  bert_encoder.bert.encoder.layer.16.attention.output.dense.weight True
##  bert_encoder.bert.encoder.layer.16.attention.output.dense.bias True
##  bert_encoder.bert.encoder.layer.16.attention.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.16.attention.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.16.intermediate.dense.weight True
##  bert_encoder.bert.encoder.layer.16.intermediate.dense.bias True
##  bert_encoder.bert.encoder.layer.16.output.dense.weight True
##  bert_encoder.bert.encoder.layer.16.output.dense.bias True
##  bert_encoder.bert.encoder.layer.16.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.16.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.17.attention.self.query.weight True
##  bert_encoder.bert.encoder.layer.17.attention.self.query.bias True
##  bert_encoder.bert.encoder.layer.17.attention.self.key.weight True
##  bert_encoder.bert.encoder.layer.17.attention.self.key.bias True
##  bert_encoder.bert.encoder.layer.17.attention.self.value.weight True
##  bert_encoder.bert.encoder.layer.17.attention.self.value.bias True
##  bert_encoder.bert.encoder.layer.17.attention.output.dense.weight True
##  bert_encoder.bert.encoder.layer.17.attention.output.dense.bias True
##  bert_encoder.bert.encoder.layer.17.attention.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.17.attention.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.17.intermediate.dense.weight True
##  bert_encoder.bert.encoder.layer.17.intermediate.dense.bias True
##  bert_encoder.bert.encoder.layer.17.output.dense.weight True
##  bert_encoder.bert.encoder.layer.17.output.dense.bias True
##  bert_encoder.bert.encoder.layer.17.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.17.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.18.attention.self.query.weight True
##  bert_encoder.bert.encoder.layer.18.attention.self.query.bias True
##  bert_encoder.bert.encoder.layer.18.attention.self.key.weight True
##  bert_encoder.bert.encoder.layer.18.attention.self.key.bias True
##  bert_encoder.bert.encoder.layer.18.attention.self.value.weight True
##  bert_encoder.bert.encoder.layer.18.attention.self.value.bias True
##  bert_encoder.bert.encoder.layer.18.attention.output.dense.weight True
##  bert_encoder.bert.encoder.layer.18.attention.output.dense.bias True
##  bert_encoder.bert.encoder.layer.18.attention.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.18.attention.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.18.intermediate.dense.weight True
##  bert_encoder.bert.encoder.layer.18.intermediate.dense.bias True
##  bert_encoder.bert.encoder.layer.18.output.dense.weight True
##  bert_encoder.bert.encoder.layer.18.output.dense.bias True
##  bert_encoder.bert.encoder.layer.18.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.18.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.19.attention.self.query.weight True
##  bert_encoder.bert.encoder.layer.19.attention.self.query.bias True
##  bert_encoder.bert.encoder.layer.19.attention.self.key.weight True
##  bert_encoder.bert.encoder.layer.19.attention.self.key.bias True
##  bert_encoder.bert.encoder.layer.19.attention.self.value.weight True
##  bert_encoder.bert.encoder.layer.19.attention.self.value.bias True
##  bert_encoder.bert.encoder.layer.19.attention.output.dense.weight True
##  bert_encoder.bert.encoder.layer.19.attention.output.dense.bias True
##  bert_encoder.bert.encoder.layer.19.attention.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.19.attention.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.19.intermediate.dense.weight True
##  bert_encoder.bert.encoder.layer.19.intermediate.dense.bias True
##  bert_encoder.bert.encoder.layer.19.output.dense.weight True
##  bert_encoder.bert.encoder.layer.19.output.dense.bias True
##  bert_encoder.bert.encoder.layer.19.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.19.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.20.attention.self.query.weight True
##  bert_encoder.bert.encoder.layer.20.attention.self.query.bias True
##  bert_encoder.bert.encoder.layer.20.attention.self.key.weight True
##  bert_encoder.bert.encoder.layer.20.attention.self.key.bias True
##  bert_encoder.bert.encoder.layer.20.attention.self.value.weight True
##  bert_encoder.bert.encoder.layer.20.attention.self.value.bias True
##  bert_encoder.bert.encoder.layer.20.attention.output.dense.weight True
##  bert_encoder.bert.encoder.layer.20.attention.output.dense.bias True
##  bert_encoder.bert.encoder.layer.20.attention.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.20.attention.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.20.intermediate.dense.weight True
##  bert_encoder.bert.encoder.layer.20.intermediate.dense.bias True
##  bert_encoder.bert.encoder.layer.20.output.dense.weight True
##  bert_encoder.bert.encoder.layer.20.output.dense.bias True
##  bert_encoder.bert.encoder.layer.20.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.20.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.21.attention.self.query.weight True
##  bert_encoder.bert.encoder.layer.21.attention.self.query.bias True
##  bert_encoder.bert.encoder.layer.21.attention.self.key.weight True
##  bert_encoder.bert.encoder.layer.21.attention.self.key.bias True
##  bert_encoder.bert.encoder.layer.21.attention.self.value.weight True
##  bert_encoder.bert.encoder.layer.21.attention.self.value.bias True
##  bert_encoder.bert.encoder.layer.21.attention.output.dense.weight True
##  bert_encoder.bert.encoder.layer.21.attention.output.dense.bias True
##  bert_encoder.bert.encoder.layer.21.attention.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.21.attention.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.21.intermediate.dense.weight True
##  bert_encoder.bert.encoder.layer.21.intermediate.dense.bias True
##  bert_encoder.bert.encoder.layer.21.output.dense.weight True
##  bert_encoder.bert.encoder.layer.21.output.dense.bias True
##  bert_encoder.bert.encoder.layer.21.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.21.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.22.attention.self.query.weight True
##  bert_encoder.bert.encoder.layer.22.attention.self.query.bias True
##  bert_encoder.bert.encoder.layer.22.attention.self.key.weight True
##  bert_encoder.bert.encoder.layer.22.attention.self.key.bias True
##  bert_encoder.bert.encoder.layer.22.attention.self.value.weight True
##  bert_encoder.bert.encoder.layer.22.attention.self.value.bias True
##  bert_encoder.bert.encoder.layer.22.attention.output.dense.weight True
##  bert_encoder.bert.encoder.layer.22.attention.output.dense.bias True
##  bert_encoder.bert.encoder.layer.22.attention.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.22.attention.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.22.intermediate.dense.weight True
##  bert_encoder.bert.encoder.layer.22.intermediate.dense.bias True
##  bert_encoder.bert.encoder.layer.22.output.dense.weight True
##  bert_encoder.bert.encoder.layer.22.output.dense.bias True
##  bert_encoder.bert.encoder.layer.22.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.22.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.23.attention.self.query.weight True
##  bert_encoder.bert.encoder.layer.23.attention.self.query.bias True
##  bert_encoder.bert.encoder.layer.23.attention.self.key.weight True
##  bert_encoder.bert.encoder.layer.23.attention.self.key.bias True
##  bert_encoder.bert.encoder.layer.23.attention.self.value.weight True
##  bert_encoder.bert.encoder.layer.23.attention.self.value.bias True
##  bert_encoder.bert.encoder.layer.23.attention.output.dense.weight True
##  bert_encoder.bert.encoder.layer.23.attention.output.dense.bias True
##  bert_encoder.bert.encoder.layer.23.attention.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.23.attention.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.23.intermediate.dense.weight True
##  bert_encoder.bert.encoder.layer.23.intermediate.dense.bias True
##  bert_encoder.bert.encoder.layer.23.output.dense.weight True
##  bert_encoder.bert.encoder.layer.23.output.dense.bias True
##  bert_encoder.bert.encoder.layer.23.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.23.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.24.attention.self.query.weight True
##  bert_encoder.bert.encoder.layer.24.attention.self.query.bias True
##  bert_encoder.bert.encoder.layer.24.attention.self.key.weight True
##  bert_encoder.bert.encoder.layer.24.attention.self.key.bias True
##  bert_encoder.bert.encoder.layer.24.attention.self.value.weight True
##  bert_encoder.bert.encoder.layer.24.attention.self.value.bias True
##  bert_encoder.bert.encoder.layer.24.attention.output.dense.weight True
##  bert_encoder.bert.encoder.layer.24.attention.output.dense.bias True
##  bert_encoder.bert.encoder.layer.24.attention.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.24.attention.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.24.intermediate.dense.weight True
##  bert_encoder.bert.encoder.layer.24.intermediate.dense.bias True
##  bert_encoder.bert.encoder.layer.24.output.dense.weight True
##  bert_encoder.bert.encoder.layer.24.output.dense.bias True
##  bert_encoder.bert.encoder.layer.24.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.24.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.25.attention.self.query.weight True
##  bert_encoder.bert.encoder.layer.25.attention.self.query.bias True
##  bert_encoder.bert.encoder.layer.25.attention.self.key.weight True
##  bert_encoder.bert.encoder.layer.25.attention.self.key.bias True
##  bert_encoder.bert.encoder.layer.25.attention.self.value.weight True
##  bert_encoder.bert.encoder.layer.25.attention.self.value.bias True
##  bert_encoder.bert.encoder.layer.25.attention.output.dense.weight True
##  bert_encoder.bert.encoder.layer.25.attention.output.dense.bias True
##  bert_encoder.bert.encoder.layer.25.attention.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.25.attention.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.25.intermediate.dense.weight True
##  bert_encoder.bert.encoder.layer.25.intermediate.dense.bias True
##  bert_encoder.bert.encoder.layer.25.output.dense.weight True
##  bert_encoder.bert.encoder.layer.25.output.dense.bias True
##  bert_encoder.bert.encoder.layer.25.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.25.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.26.attention.self.query.weight True
##  bert_encoder.bert.encoder.layer.26.attention.self.query.bias True
##  bert_encoder.bert.encoder.layer.26.attention.self.key.weight True
##  bert_encoder.bert.encoder.layer.26.attention.self.key.bias True
##  bert_encoder.bert.encoder.layer.26.attention.self.value.weight True
##  bert_encoder.bert.encoder.layer.26.attention.self.value.bias True
##  bert_encoder.bert.encoder.layer.26.attention.output.dense.weight True
##  bert_encoder.bert.encoder.layer.26.attention.output.dense.bias True
##  bert_encoder.bert.encoder.layer.26.attention.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.26.attention.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.26.intermediate.dense.weight True
##  bert_encoder.bert.encoder.layer.26.intermediate.dense.bias True
##  bert_encoder.bert.encoder.layer.26.output.dense.weight True
##  bert_encoder.bert.encoder.layer.26.output.dense.bias True
##  bert_encoder.bert.encoder.layer.26.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.26.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.27.attention.self.query.weight True
##  bert_encoder.bert.encoder.layer.27.attention.self.query.bias True
##  bert_encoder.bert.encoder.layer.27.attention.self.key.weight True
##  bert_encoder.bert.encoder.layer.27.attention.self.key.bias True
##  bert_encoder.bert.encoder.layer.27.attention.self.value.weight True
##  bert_encoder.bert.encoder.layer.27.attention.self.value.bias True
##  bert_encoder.bert.encoder.layer.27.attention.output.dense.weight True
##  bert_encoder.bert.encoder.layer.27.attention.output.dense.bias True
##  bert_encoder.bert.encoder.layer.27.attention.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.27.attention.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.27.intermediate.dense.weight True
##  bert_encoder.bert.encoder.layer.27.intermediate.dense.bias True
##  bert_encoder.bert.encoder.layer.27.output.dense.weight True
##  bert_encoder.bert.encoder.layer.27.output.dense.bias True
##  bert_encoder.bert.encoder.layer.27.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.27.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.28.attention.self.query.weight True
##  bert_encoder.bert.encoder.layer.28.attention.self.query.bias True
##  bert_encoder.bert.encoder.layer.28.attention.self.key.weight True
##  bert_encoder.bert.encoder.layer.28.attention.self.key.bias True
##  bert_encoder.bert.encoder.layer.28.attention.self.value.weight True
##  bert_encoder.bert.encoder.layer.28.attention.self.value.bias True
##  bert_encoder.bert.encoder.layer.28.attention.output.dense.weight True
##  bert_encoder.bert.encoder.layer.28.attention.output.dense.bias True
##  bert_encoder.bert.encoder.layer.28.attention.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.28.attention.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.28.intermediate.dense.weight True
##  bert_encoder.bert.encoder.layer.28.intermediate.dense.bias True
##  bert_encoder.bert.encoder.layer.28.output.dense.weight True
##  bert_encoder.bert.encoder.layer.28.output.dense.bias True
##  bert_encoder.bert.encoder.layer.28.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.28.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.29.attention.self.query.weight True
##  bert_encoder.bert.encoder.layer.29.attention.self.query.bias True
##  bert_encoder.bert.encoder.layer.29.attention.self.key.weightSome weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'cls.predictions.decoder.bias', 'bert.pooler.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
 True
##  bert_encoder.bert.encoder.layer.29.attention.self.key.bias True
##  bert_encoder.bert.encoder.layer.29.attention.self.value.weight True
##  bert_encoder.bert.encoder.layer.29.attention.self.value.bias True
##  bert_encoder.bert.encoder.layer.29.attention.output.dense.weight True
##  bert_encoder.bert.encoder.layer.29.attention.output.dense.bias True
##  bert_encoder.bert.encoder.layer.29.attention.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.29.attention.output.LayerNorm.bias True
##  bert_encoder.bert.encoder.layer.29.intermediate.dense.weight True
##  bert_encoder.bert.encoder.layer.29.intermediate.dense.bias True
##  bert_encoder.bert.encoder.layer.29.output.dense.weight True
##  bert_encoder.bert.encoder.layer.29.output.dense.bias True
##  bert_encoder.bert.encoder.layer.29.output.LayerNorm.weight True
##  bert_encoder.bert.encoder.layer.29.output.LayerNorm.bias True
##  projection.0.weight True
##  projection.0.bias True
##  projection.2.weight True
##  projection.2.bias True
##  projection.3.weight True
##  projection.3.bias True
##  projection.5.weight True
##  projection.5.bias True
##  projection.7.weight True
##  projection.7.bias True
##  projection.9.weight True
##  projection.9.bias True
##  projection.10.weight True
##  projection.10.bias True
##  projection.12.weight True
##  projection.12.bias True
##  projection.14.weight True
##  projection.14.bias True
##  projection.16.weight True
##  projection.16.bias True
##  projection.17.weight True
##  projection.17.bias True
##  projection.19.weight True
##  projection.19.bias True
##  head.weight True
##  head.bias True
0       0
1       0
2       0
3       0
4       0
       ..
1466    1
1467    1
1468    1
1469    1
1470    1
Name: label, Length: 1471, dtype: int64
test dataset:
 Class 0: 52
 Class 1: 52
train dataset:
 Class 0: 208
 Class 1: 208
Using device cuda:0
Classification_dim: 256
Epoch 1/15
##########
## Train
tensor([[ 0.1354,  0.7237],
        [-0.6435, -0.6361],
        [-0.4246,  0.7071],
        [-0.3903,  0.0725],
        [-0.3747, -0.4303],
        [ 0.1499, -0.0513],
        [-0.5713, -0.0769],
        [-0.6711,  0.0827]], device='cuda:0', grad_fn=<AddmmBackward0>)
train 0/52 Loss: 0.8810 Running Loss: 0.0169 Acc: 0.0072 MCC: -0.1491
tensor([[ 0.0388, -1.3524],
        [ 0.4810, -1.0298],
        [ 0.2212, -0.9278],
        [ 0.2111, -0.5063],
        [ 0.6360, -0.1970],
        [ 0.0722, -0.2934],
        [ 0.8114, -1.6837],
        [ 1.3751, -1.2343]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[ 0.1140, -0.7128],
        [ 0.3802, -0.1838],
        [-0.3809, -0.4843],
        [-0.3637,  0.0932],
        [-0.3382, -0.2154],
        [-0.1018, -0.5164],
        [-0.5838, -0.8298],
        [-0.3529,  0.2901]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[-0.8400,  0.5879],
        [-0.9908,  0.2237],
        [-0.1360,  0.1994],
        [-0.5370,  0.8157],
        [-1.2307,  0.5306],
        [-0.5502,  0.5321],
        [-0.4725,  0.6681],
        [-0.9386,  0.1133]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[-0.9892,  0.7770],
        [-0.7252,  0.6661],
        [-0.6313,  0.4434],
        [-0.7269, -0.0587],
        [-0.4416,  1.0619],
        [-0.9489,  0.7559],
        [-0.8181,  0.7066],
        [-0.8144,  0.5296]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[ 0.3299, -0.1377],
        [-0.8715,  0.1178],
        [ 0.1292, -0.4605],
        [ 0.2197, -0.2782],
        [-0.6316,  0.8197],
        [ 0.4952,  0.2402],
        [ 0.0873, -0.3446],
        [-0.2275,  0.1235]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[ 0.3293, -0.1715],
        [ 0.6560, -0.4216],
        [ 1.3420, -0.6708],
        [ 0.8123, -0.3158],
        [ 0.9636, -0.9997],
        [ 1.0776, -0.5121],
        [ 0.9371, -0.7224],
        [ 0.8046, -0.3692]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[ 1.1127, -0.5437],
        [ 0.3758, -0.5639],
        [ 0.9087, -0.3208],
        [ 0.4936, -0.4361],
        [ 0.5978, -0.8114],
        [ 1.5286, -0.8676],
        [ 0.3625, -0.5910],
        [ 1.0561, -0.8167]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[-0.2304, -0.5578],
        [ 0.3556, -0.3067],
        [ 0.7952, -0.9460],
        [-0.3712, -0.7839],
        [ 0.0702, -0.3207],
        [ 0.7736, -0.7441],
        [ 0.4202, -0.2257],
        [ 0.5158, -0.6267]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[-0.3233, -0.1303],
        [ 0.1213,  0.0131],
        [-0.1192, -0.5479],
        [-0.3804, -0.2558],
        [ 0.0778, -0.1120],
        [ 0.0823,  0.4009],
        [ 0.0960,  0.0720],
        [-0.0023,  0.0021]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[-0.2898,  0.5177],
        [-1.0234,  0.3750],
        [-0.1803,  0.3017],
        [-0.7678,  0.5403],
        [-0.6986,  0.7694],
        [-0.3089,  0.2753],
        [-0.3869,  0.6923],
        [-0.5085,  0.5514]], device='cuda:0', grad_fn=<AddmmBackward0>)
train 10/52 Loss: 0.8510 Running Loss: 0.1702 Acc: 0.1106 MCC: 0.0456
tensor([[-0.9714,  1.1645],
        [-0.6560,  0.7277],
        [-0.6784,  0.1682],
        [-0.6465,  0.6309],
        [-0.8732,  0.5996],
        [-0.7852,  0.2401],
        [-0.7215,  0.5595],
        [-0.7625,  0.7640]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[-0.5981,  0.3017],
        [-0.4594,  0.2243],
        [-0.7328,  0.4762],
        [-0.5106,  0.3076],
        [-0.1025,  0.0993],
        [-0.1060,  0.8072],
        [-1.0063,  0.7172],
        [-0.4993,  0.4415]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[-0.3308, -0.0158],
        [-0.1352, -0.3069],
        [ 0.0425,  0.2189],
        [-0.5266, -0.2078],
        [ 0.6495,  0.1621],
        [ 0.2022,  0.4142],
        [-0.6760, -0.2201],
        [-0.1312,  0.0991]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[ 0.2657, -0.3695],
        [ 0.3141, -0.5388],
        [ 0.4397,  0.1296],
        [ 0.0077, -0.8705],
        [ 0.9741, -0.5096],
        [ 0.3896, -0.1027],
        [-0.1394, -0.5349],
        [ 0.3664, -0.8035]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[-0.0715, -0.4517],
        [ 0.6033, -0.5756],
        [ 0.3245, -0.9254],
        [ 0.5217, -0.6128],
        [ 0.7770,  0.0688],
        [ 0.4749, -0.1827],
        [ 0.7384, -0.4955],
        [ 0.5026, -0.5814]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[ 0.8277, -0.8735],
        [ 0.5891, -1.7995],
        [ 1.2454, -0.0534],
        [ 0.7795, -0.7576],
        [ 0.9176, -0.9970],
        [ 1.0323, -0.6856],
        [ 0.3148, -0.4973],
        [ 0.6449, -0.9554]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[ 0.1039, -0.4897],
        [ 0.8920, -0.0639],
        [-0.1789, -0.2911],
        [ 0.2210, -0.0885],
        [ 0.3915, -0.3616],
        [ 0.8371,  0.0979],
        [ 0.6630, -0.1884],
        [ 0.4823, -1.2091]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[-1.4620e-01,  2.2803e-01],
        [ 6.3471e-01, -3.9447e-01],
        [ 3.1948e-01,  4.4623e-04],
        [-4.7274e-02, -1.8230e-01],
        [ 8.6497e-02, -2.9524e-01],
        [-2.0215e-01, -3.8526e-01],
        [ 2.7401e-01, -4.7850e-01],
        [-9.6220e-02,  7.8195e-01]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[ 0.4869, -0.3390],
        [ 0.5071, -0.0410],
        [ 0.2335, -0.1550],
        [-0.0490,  0.7244],
        [ 0.3652, -0.1704],
        [ 0.1065, -0.4882],
        [ 0.2338, -0.2030],
        [-0.1315, -0.1721]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[ 0.1676,  0.0681],
        [-0.3715,  0.0485],
        [-0.2708,  0.2360],
        [-0.1863, -0.0755],
        [ 0.3350, -0.4977],
        [-0.4840,  0.1895],
        [-0.0146, -0.4096],
        [-0.2496, -0.4621]], device='cuda:0', grad_fn=<AddmmBackward0>)
train 20/52 Loss: 0.7567 Running Loss: 0.3043 Acc: 0.2236 MCC: 0.1004
tensor([[-0.1061,  0.1429],
        [-0.0955,  0.0152],
        [-0.4733, -0.4009],
        [-0.0825, -0.0803],
        [-0.2889,  0.2369],
        [-0.1059, -0.2416],
        [-0.2725, -0.5474],
        [ 0.0381, -0.6023]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[-0.0880, -0.2519],
        [-0.1169, -0.0275],
        [ 0.1043, -0.4065],
        [ 0.2928, -0.4761],
        [-0.1045,  0.3438],
        [ 0.5257, -0.0162],
        [ 0.1342, -0.0027],
        [ 0.4976,  0.2788]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[ 0.5372, -0.2325],
        [ 0.4807, -0.4383],
        [ 0.3181, -0.7343],
        [ 0.4174,  0.0104],
        [ 0.5229, -0.5158],
        [ 0.1940, -0.3016],
        [-0.0348, -0.5663],
        [ 0.3535, -0.7775]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[ 0.1619, -0.1295],
        [ 0.2904,  0.2537],
        [ 0.2188, -0.1934],
        [ 0.0605, -0.6067],
        [-0.1205,  0.0043],
        [ 0.4754, -0.3318],
        [ 0.1011,  0.1094],
        [ 0.6886, -0.1084]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[-0.0411,  0.0556],
        [ 0.0689, -0.6186],
        [ 0.1457, -0.0158],
        [-0.2996, -0.0014],
        [-0.2336, -0.1852],
        [-0.2987,  0.0818],
        [-0.0886, -0.2586],
        [ 0.2974,  0.2651]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[ 0.1877,  0.1256],
        [-0.3605,  0.2795],
        [ 0.0735,  0.1021],
        [-0.2165,  0.4120],
        [-0.2191,  0.2279],
        [ 0.2285,  0.1352],
        [-0.2371,  0.4175],
        [-0.3278, -0.1495]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[-0.0358, -0.0628],
        [-0.3166,  0.3746],
        [-0.2786, -0.2090],
        [-0.1973,  0.3066],
        [-0.0104,  0.4741],
        [-0.4402,  0.0520],
        [-0.2916, -0.0950],
        [-0.6029,  0.4733]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[-0.3288,  0.3577],
        [-0.3512,  0.5612],
        [-0.2245,  0.2545],
        [-0.3554,  0.1147],
        [-0.6499,  0.2856],
        [ 0.0408,  0.6135],
        [-0.0413,  0.3243],
        [-0.5326,  0.5477]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[-0.3941,  0.2580],
        [ 0.1562,  0.2413],
        [-0.7272,  0.4683],
        [-0.5557,  0.2462],
        [-0.4149,  0.0773],
        [-0.5359, -0.0215],
        [-0.2327,  0.4195],
        [-0.3461,  0.3464]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[-0.1564,  0.1862],
        [ 0.0285,  0.3593],
        [-0.0832,  0.0180],
        [ 0.3145,  0.2512],
        [ 0.1888, -0.1350],
        [-0.2031,  0.7225],
        [-0.1217,  0.1166],
        [ 0.0404, -0.1242]], device='cuda:0', grad_fn=<AddmmBackward0>)
train 30/52 Loss: 0.6488 Running Loss: 0.4456 Acc: 0.3149 MCC: 0.0540
tensor([[ 0.1383, -0.0781],
        [ 0.0257, -0.0259],
        [ 0.0052, -0.0132],
        [ 0.2236, -0.1519],
        [-0.1884, -0.2432],
        [ 0.0526, -0.0213],
        [-0.3177, -0.1120],
        [ 0.2787, -0.2369]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[ 0.2774, -0.5547],
        [ 0.4105,  0.0954],
        [ 0.4299, -0.0648],
        [ 0.3776, -0.4451],
        [ 0.0896,  0.1686],
        [ 0.0095,  0.2481],
        [ 0.6527, -0.0764],
        [ 0.2023, -0.2224]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[ 0.4301, -0.6599],
        [ 0.2732, -0.3978],
        [-0.1358, -0.3556],
        [ 0.2434, -0.2138],
        [ 0.1873, -0.6169],
        [-0.1366,  0.1445],
        [-0.0340, -0.4195],
        [ 0.0435, -0.6105]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[ 0.1177, -0.1845],
        [-0.2098, -0.1736],
        [ 0.6738, -0.5109],
        [ 0.1421, -0.3443],
        [ 0.5522, -0.1676],
        [-0.1590,  0.2389],
        [ 0.2496, -0.4404],
        [ 0.5524, -0.3311]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[-0.4124,  0.3675],
        [ 0.2214,  0.0800],
        [ 0.1691,  0.0112],
        [ 0.1483, -0.4114],
        [-0.1014, -0.3785],
        [-0.0687,  0.1819],
        [ 0.3986, -0.2852],
        [ 0.2010, -0.0163]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[-0.5990,  0.1286],
        [ 0.0966,  0.0568],
        [-0.4989,  0.1938],
        [-0.0034, -0.0675],
        [ 0.0536, -0.1107],
        [ 0.0764, -0.2882],
        [-0.2237,  0.0507],
        [ 0.0325, -0.1319]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[-0.2359,  0.5418],
        [-0.2342,  0.5055],
        [-0.3828,  0.4345],
        [-0.5721,  0.2128],
        [-0.0250,  0.3478],
        [-0.0394,  0.3479],
        [ 0.1170,  0.4828],
        [-0.1573,  0.0150]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[-0.3549,  0.1591],
        [-0.3610,  0.5593],
        [-0.5491,  0.3577],
        [-0.4813,  0.4482],
        [-0.2664,  0.3179],
        [-0.4382,  0.1609],
        [-0.3734,  0.4320],
        [-0.5929,  0.1633]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[-0.4112,  0.6264],
        [-0.4624,  0.7638],
        [-0.5101,  0.8176],
        [-0.6545,  0.7527],
        [-1.1190,  0.5254],
        [-0.6887,  0.7392],
        [-0.7099,  0.4365],
        [-0.8031,  0.7749]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[-0.6258,  0.5617],
        [-0.7653,  0.6875],
        [-0.2825,  0.3588],
        [-0.6631,  0.5091],
        [-0.8629,  0.5426],
        [-0.4700,  1.2350],
        [-0.7726,  0.5951],
        [-0.8426,  0.6291]], device='cuda:0', grad_fn=<AddmmBackward0>)
train 40/52 Loss: 0.8143 Running Loss: 0.5945 Acc: 0.4062 MCC: 0.0305
tensor([[-0.5560,  0.3747],
        [-0.3298,  0.6549],
        [-0.3929,  0.3859],
        [-0.3560,  0.5595],
        [-0.2408,  0.7261],
        [-0.4844,  0.5262],
        [-0.7427,  0.7498],
        [-0.3166, -0.0102]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[-0.4003,  0.2596],
        [-0.6559,  0.2327],
        [-0.2737,  0.3166],
        [-0.3909,  0.2876],
        [-0.5187,  0.4545],
        [-0.1693,  0.0368],
        [-0.3515,  0.3181],
        [-0.4699,  0.2743]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[-0.1875,  0.3984],
        [-0.4944,  0.3078],
        [-0.4962, -0.0984],
        [-0.6302,  0.6461],
        [-0.4186,  0.1893],
        [-0.0557,  0.2141],
        [-0.2550,  0.0959],
        [-0.4224,  0.2788]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[ 0.0198, -0.1068],
        [-0.1569, -0.1511],
        [ 0.0848, -0.0590],
        [-0.0017,  0.1250],
        [ 0.1581,  0.0798],
        [-0.1894, -0.0196],
        [-0.0383, -0.0531],
        [ 0.2804,  0.2480]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[-1.6050e-01, -2.4634e-01],
        [ 2.1112e-01, -2.5894e-01],
        [ 1.2058e-01, -4.6987e-01],
        [ 1.6281e-01, -1.5837e-01],
        [ 3.0171e-01, -2.2944e-01],
        [ 4.9034e-01, -3.8357e-01],
        [ 5.8811e-01,  1.0972e-04],
        [ 1.0897e-01,  5.9918e-02]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[ 0.2189, -0.5014],
        [ 0.5378, -0.4875],
        [ 0.5461, -0.2712],
        [ 0.3102, -0.6174],
        [ 0.4857, -0.8209],
        [ 0.4513, -0.5980],
        [ 0.3048, -0.3039],
        [ 0.4840, -0.8438]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[ 0.7491, -0.5353],
        [ 0.6144, -0.5817],
        [ 0.6395, -0.5121],
        [ 0.6278, -0.5547],
        [ 0.6860, -0.9234],
        [ 0.5050, -0.5523],
        [ 0.6802, -0.7262],
        [ 0.6030, -0.6866]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[ 0.3197, -0.4678],
        [ 0.3506, -0.7282],
        [ 0.9288, -0.3916],
        [ 0.6948, -0.6595],
        [ 0.2770, -0.5920],
        [ 0.3303, -0.3558],
        [ 0.5736, -0.1758],
        [ 0.2856, -0.6712]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[ 0.4105, -0.1755],
        [ 0.4092, -0.3636],
        [ 0.2921, -0.1227],
        [ 0.3237, -0.3817],
        [ 0.4277, -0.4993],
        [ 0.1965, -0.4890],
        [ 0.5751, -0.3586],
        [ 0.3319, -0.2956]], device='cuda:0', grad_fn=<AddmmBackward0>)
tensor([[ 0.2101, -0.0064],
        [-0.1413, -0.1698],
        [ 0.2006, -0.2613],
        [ 0.1740, -0.2095],
        [-0.1288, -0.0869],
        [ 0.2204,  0.0880],
        [ 0.2322,  0.0273],
        [ 0.0598, -0.0590]], device='cuda:0', grad_fn=<AddmmBackward0>)
train 50/52 Loss: 0.7170 Running Loss: 0.7395 Acc: 0.5048 MCC: 0.0299
tensor([[-0.1699,  0.0874],
        [-0.0572, -0.1495],
        [-0.1831, -0.1079],
        [-0.0486,  0.2087],
        [-0.1941,  0.4938],
        [-0.0192,  0.2096],
        [-0.1821, -0.0727],
        [-0.0590,  0.2904]], device='cuda:0', grad_fn=<AddmmBackward0>)
train 51/52 Loss: 0.7331 Running Loss: 0.7536 Acc: 0.5144 MCC: 0.0289
## Test
